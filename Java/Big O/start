Big 0, Big Theta, and Big Omega
=============================== 
If you've never covered big O in an academic setting, you can probably skip this subsection. It might
confuse you more than it helps. This "FYI" is mostly here to clear up ambiguity in wording for people who
have learned big O before, so that they don't say, "But I thought big O meant..:'

Academics use big O, big 0 (theta), and big Ω (omega) to describe runtimes.
O (big O): In academia, big O describes an upper bound on the time. An algorithm that prints all the
values in an array could be described as O(N), but it could also be described as O(N^2), O(N^3), or 0(2^N)
(or many other big O times). The algorithm is at least as fast as each of these; therefore they are upper
bounds on the runtime. This is similar to a less-than-or-equal-to relationship. If Bob is X years old (I'll
assume no one lives past age 130), then you could say X<=130. It would also be correct to say that
X<=1,000 or X<=1,000,000. It's technically true (although not terribly useful). Likewise, a simple
algorithm to print the values in an array is O(N) as well as O(N raisedTo 3) or any runtime bigger than O(N).

Ω (big omega): In academia, Ω is the equivalent concept but for lower bound. Printing the values in
an array is Ω(N) as well as Ω(log N) and Ω(1). After all, you know that it won't be faster than those
runtimes.

0 (big theta): In academia, 0 means both O and Ω. That is, an algorithm is 0(N) if it is both O(N) and
Ω(N). 0 gives a tight bound on runtime.

In industry (and therefore in interviews), people seem to have merged 0 and O together. Industry's meaning
of big O is closer to what academics mean by 0, in that it would be seen as incorrect to describe printing an
array as O(N^2). Industry would just say this is O(N).
For this book, we will use big O in the way that industry tends to use it: By always trying to offer the tightest
description of the runtime.

Best, Worst & Expected
======================

Best Case: If all elements are equal, then quick sort will, on average, just traverse through the array once.
This is O(N). (This actually depends slightly on the implementation of quick sort. There are implementations,
though, that will run very quickly on a sorted array.)

Worst Case: What if we get really unlucky and the pivot is repeatedly the biggest element in the array?
(Actually, this can easily happen. If the pivot is chosen to be the first element in the subarray and the
array is sorted in reverse order, we'll have this situation.) In this case, our recursion doesn't divide the
array in half and recurse on each half. It just shrinks the subarray by one element. This will degenerate
to an O(N^2) runtime.

Expected Case: Usually, though, these wonderful or terrible situations won't happen. Sure, sometimes
the pivot will be very low or very high, but it won't happen over and over again. We can expect a runtime
of O(N log N).

We rarely ever discuss best case time complexity, because it's not a very useful concept. After all, we could
take essentially any algorithm, special case some input, and then get an O (1) time in the best case.
For many-probably most-algorithms, the worst case and the expected case are the same. Sometimes
they're different, though, and we need to describe both of the runtimes.

What is the relationship between best/worst/expected case and big O/theta/omega?
It's easy for candidates to muddle these concepts (probably because both have some concepts of"higher",
"lower" and "exactly right"), but there is no particular relationship between the concepts.
Best, worst, and expected cases describe the big O (or big theta) time for particular inputs or scenarios.

Big O, big omega, and big theta describe the upper, lower, and tight bounds for the runtime.

Amortized Time
==============

An Array List, or a dynamically resizing array, allows you to have the benefits of an array while offering
flexibility in size. You won't run out of space in the Arraylist since its capacity will grow as you insert
elements.

An Arraylist is implemented with an array. When the array hits capacity, the Arraylist class will create a
new array with double the capacity and copy all the elements over to the new array.
How do you describe the runtime of insertion? This is a tricky question.
The array could be full. If the array contains N elements, then inserting a new element will take O(N) time.
You will have to create a new array of size 2N and then copy N elements over. This insertion will take O(N)
time.
However, we also know that this doesn't happen very often. The vast majority of the time insertion will be
in O(l) time.
We need a concept that takes both into account. This is what amortized time does. It allows us to describe
that, yes, this worst case happens every once in a while. But once it happens, it won't happen again for so
long that the cost is "amortized"

In this case, what is the amortized time?
As we insert elements, we double the capacity when the size of the array is a power of 2. So after X elements,
we double the capacity at array sizes 1, 2, 4, 8, 16, ... , X. That doubling takes, respectively, 1, 2, 4, 8, 16, 32,
64, ... , X copies.
What is the sum of 1 + 2 + 4 + 8 + 16 + ... + X? If you read this sum left to right, it starts with 1 and doubles
until it gets to X. If you read right to left, it starts with X and halves until it gets to 1.
What then is the sum of X + X/2 + X/4 + X/8 + ... + 1 ?This is roughly 2X.
Therefore, X insertions take O(2X) time. The amortized time for each insertion is O(1).

Log N Runtimes
===============
We commonly see O(log N) in runtimes. Where does this come from?
Let's look at binary search as an example. In binary search, we are looking for an example x in an N-element
sorted array. We first compare x to the midpoint of the array. If x == middle, then we return. If x <
middle, then we search on the left side of the array. If x > middle, then we search on the right side of
the array.

search 9 within {1, 5, 8, 9, 11, 13, 15, 19, 21}
  compare 9 to 11 -> smaller.
search 9 within {1, 5, 8, 9, 11}
  compare 9 to 8 -> bigger
search 9 within {9, 11}
  compare 9 to 9
return

We start off with an N-element array to search. Then, after a single step, we're down to Yi elements. One
more step, and we're down to % elements. We stop when we either find the value or we're down to just
one element.
The total runtime is then a matter of how many steps (dividing N by 2 each time) we can take until N
becomes 1.
N = 16
N = 8 /* divide by 2 */
N = 4 /* divide by 2 *I
N = 2 /* divide by 2 */
N = 1 /* divide by 2 */
We could look at this in reverse (going from 1 to 16 instead of 16 to 1 ). How many times we can multiply 1
by 2 until we get N?
N = 1
N = 2 /* multiply by 2 */
N = 4 /* multiply by 2 */
N = 8 /* multiply by 2 */
N = 16 /* multiply by 2 */

What is k in the expression 2 raisedTo k = N?This is exactly what log expresses.
2 raisedTo 4 = 16 -> log base(2) l6 = 4
log base(2) N = k -> 2 raisedTo k = N
This is a good takeaway for you to have. When you see a problem where the number of elements in the
problem space gets halved each time, that will likely be a O(log N) runtime.
This is the same reason why finding an element in a balanced binary search tree is O(log N). With each
comparison, we go either left or right. Half the nodes are on each side, so we cut the problem space in half
each time.
I What's the base of the log? That's an excellent question!The short answer is that it doesn't matter
for the purposes of big 0. The longer explanation can be found at "Bases of Logs" on page 630.

Bases of Logs
=============
Suppose we have something in log2 (log base 2). How do we convert that to log10 (log base 10)? That is, what's the
relationship between (logb k) and (logx k) ?

Let's do some math. Assume c=logb k lets express logb k in terms of logx k.

logb k = c 
b^c = k                // This is the definition of log.
logx (b^c) = logx k    // Take log of both sides of b^c = k.
c logx b = logx k      // Rules of logs. You can move out the exponents.
c = logx k / logx b
c = logb k

Therefore, if we want to convert log2 p to log10 p, we just do this:
c = log10 p
x = 2
k = p
b = 10

log10 p = log2 p / log2 10
so, log10 p = constant * log2 p

Takeaway: Logs of different bases are only off by a constant factor. For this reason, we largely ignore what
the base of a log within a big O expression. It doesn't matter since we drop constants anyway.

Recursive Runtimes
==================
Here's a tricky one. What's the runtime of this code?
1 int f(int n) {
2   if (n <= 1) {
3     return 1;
4   } 
5   return f(n - 1) + f(n - 1);
6 }

A lot of people will, for some reason, see the two calls to f and jump to O(N raisedTo 2). This is completely incorrect.
Rather than making assumptions, let's derive the runtime by walking through the code. Suppose we call
f(4). This calls f(3) twice. Each of those calls to f(3) calls f(2), until we get down to f(1).

                   f(4)
        f(3)                   f(3)
  f(2)       f(2)        f(2)       f(2) 
f(l) f(l) f(l)  f(l)  f(l) f(l)   f(l) f(l)

How many calls are in this tree? (Don't count!)
The tree will have depth N. Each node (i.e., function call) has two children. Therefore, each level will have
twice as many calls as the one above it. The number of nodes on each level is:
Level #Nodes  Also expressed as                                         or
0     1                                                                 2 rasiedTo 0
1     2      2 * previous level = 2                                     2 rasiedTo 1
2     4      2 * previous level = 2 * 2 rasiedTo 1 = 2 rasiedTo 2       2 rasiedTo 2
3     8      2 * previous level = 2 * 2 rasiedTo 2 = 2 rasiedTo 3       2 rasiedTo 3
4     16     2 * previous level = 2 * 2 rasiedTo 3 = 2 rasiedTo 4       2 rasiedTo 4
Therefore, there will be 
2 rasiedTo 0 + 2 rasiedTo 1 + 2rasiedTo 2 + 2 rasiedTo 3 + 2 rasiedTo 4 + . • . 
+ 2 rasiedTo N (which is 2 rasiedTo (N+1) - 1) nodes. (See "Sum of Powers of 2" on page 630.)

Try to remember this pattern. When you have a recursive function that makes multiple calls, the runtime will
often (but not always) look like O(branches rasiedTo depth), where branches is the number of times each recursive
call branches. In this case, this gives us O(2 rasiedTo N).

As you may recall, the base of a log doesn't matter for big O since logs of different bases are
only different by a constant factor. However, this does not apply to exponents. The base of an
exponent does matter. Compare 2 rasiedTo n and 8 rasiedTo n. If you expand 8 rasiedTo n, you get ((2 rasiedTo 3) rasiedTo n), 
which equals 2 rasiedTo 3n,
which equals (2 rasiedTo 2n) * (2 rasiedTo n). As you can see, (8 rasiedTo n)  and (2 rasiedTo n) are different 
by a factor of (2 rasiedTo 2n). That is very much not a constant factor!

The space complexity of this algorithm will be O(N). Although we have O(2N) nodes in the tree total, only
O(N) exist at any given time. Therefore, we would only need to have O(N) memory available.

Example 13
===========
The following code computes the Nth Fibonacci number.
1 int fib(int n) {
2   if (n <= 0) return 0;
3   else if (n == 1) return 1;
4   return fib(n - 1) + fib(n - 2);
5 }
We can use the earlier pattern we'd established for recursive calls: O(branches rasiedTo depth).
There are 2 branches per call, and we go as deep as N, therefore the runtime is O(2 raisedTo N).

Through some very complicated math, we can actually get a tighter runtime. The time is indeed
exponential, but it's actually closer to O(1.6 raisedTo N). The reason that it's not exactly O(2 raisedTo N) is that, at
the bottom of the call stack, there is sometimes only one call. It turns out that a lot of the nodes
are at the bottom (as is true in most trees), so this single versus double call actually makes a big
difference. Saying O(2 raisedTo N) would suffice for the scope of an interview, though (and is still technically
correct, if you read the note about big theta on page 39). You might get "bonus points" if
you can recognize that it'll actually be less than that.

Generally speaking, when you see an algorithm with multiple recursive calls, you're looking at exponential
runtime.

Example14
==========
The following code prints all Fibonacci numbers from 0 to n. What is its time complexity?
1 void allFib(int n) {
2   for (int i= 0; i < n; i++) {
3     System.out.println(i + ": "+ fib(i));
4   }
5 }
6
7 int fib(int n) {
8   if (n <= 0) 
      return 0;
9   else if (n == 1) return 1;
10    return fib(n - 1) + fib(n - 2);
11 }

Many people will rush to concluding that since fib(n) takes O(2 rasiedTo n) time and it's called n times, then it's
O(n *(2 raisedTo n)).
Not so fast. Can you find the error in the logic?
The error is that the n is changing. Yes, fib (n) takes 0(2 raisedTo n) time, but it matters what that value of n is.
Instead, let's walk through each call.
fib(1) -> 2 raisedTo 1 steps
fib(2) -> 2 raisedTo 2 steps
fib(3) -> 2 raisedTo 3 steps
fib(4) -> 2 raisedTo 4 steps
fib(n) -> 2 raisedTo n steps

Therefore, the total amount of work is:
2 raisedTo 1 + 2 raisedTo 2 + 2 raisedTo 3 + 2 raisedTo 4 + , , , + 2 raisedTo n
As we showed on page 44, this is (2 raisedTo n+1). Therefore, the runtime to compute the first n Fibonacci numbers
(using this terrible algorithm) is still O(2 raisedTo n).

Above is by geometric progression formula:

For series:
{ar^0, ar^1, ar^2, ar^3, .... ar^(n-1)}
where a = first term, r = common ratio, n = number of terms
- nth term Xn = ar^(n-1)
- The sum of series:
ar^0 + ar^1 + ar^2 + ar^3 .... + ar^(n-1) = a(1-r^n)/(1-r)

Arithematic progression:
For series:
{a, a+d, a+2d, a+3d, ... }
where a = first term, d = common difference
- nth term Xn = a+(n-1)d
- The sum of series:
a + (a+d) + (a+2d) + (a+3d) ... + (a+(n-1)d) = (n/2)*(2a+(n-1)d)


Example 15
==========
The following code prints all Fibonacci numbers from 0 to n. However, this time, it stores (i.e., caches) previously
computed values in an integer array. If it has already been computed, it just returns the cache. What is its runtime?
1 void allFib(int n) {
2   int[] memo = new int[n + 1];
3   for (int i= 0; i < n; i++) {
4     System.out.println(i + ": "+ fib(i, memo));
5   }
6 }
7
8 int fib(int n, int[] memo) {
9   if (n <= 0) 
       return 0;
10  else if (n == 1)
       return 1;
11  else if (memo[n] > 0) 
       return memo[n];
13  memo[n] = fib(n - 1, memo)+ fib(n - 2, memo);     //memoization
14  return memo[n];
15 }
Let's walk through what this algorithm does.
fib(l) -> return 1
 fib(2)
  fib(l) -> return 1
  fib(0) -> return 0
  store 1 at memo[2]
fib(3)
  fib(2) -> lookup memo[2] -> return 1
  fib(l) -> return 1
  store 2 at memo[3]
fib(4)
  fib(3) -> lookup memo[3] -> return 2
  fib(2) -> lookup memo[2] -> return 1
  store 3 at memo[4]
fib(S)
  fib(4) -> lookup memo[4] -> return 3
  fib(3) -> lookup memo[3] -> return 2
  store 5 at memo[5]
  
At each call to fib(i), we have already computed and stored the values for fib(i-1) and fib(i-2).
We just look up those values, sum them, store the new result, and return. This takes a constant amount of
time.
We're doing a constant amount of work N times, so this is O (n) time.
This technique, called memoization, is a very common one to optimize exponential time recursive algorithms.
