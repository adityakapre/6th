Big 0, Big Theta, and Big Omega
=============================== 
If you've never covered big O in an academic setting, you can probably skip this subsection. It might
confuse you more than it helps. This "FYI" is mostly here to clear up ambiguity in wording for people who
have learned big O before, so that they don't say, "But I thought big O meant..:'

Academics use big O, big 0 (theta), and big Ω (omega) to describe runtimes.
O (big O): In academia, big O describes an upper bound on the time. An algorithm that prints all the
values in an array could be described as O(N), but it could also be described as O(N2), O(N3), or 0( 2N)
(or many other big O times). The algorithm is at least as fast as each of these; therefore they are upper
bounds on the runtime. This is similar to a less-than-or-equal-to relationship. If Bob is X years old (I'll
assume no one lives past age 130), then you could say X<=130. It would also be correct to say that
X<=1,000 or X<=1,000,000. It's technically true (although not terribly useful). Likewise, a simple
algorithm to print the values in an array is O(N) as well as O(N raisedTo 3) or any runtime bigger than O(N).

Ω (big omega): In academia, Ω is the equivalent concept but for lower bound. Printing the values in
an array is Ω(N) as well as Ω(log N) and Ω(1). After all, you know that it won't be faster than those
runtimes.

0 (big theta): In academia, 0 means both O and Ω. That is, an algorithm is 0(N) if it is both O(N) and
Ω( N). 0 gives a tight bound on runtime.

In industry (and therefore in interviews), people seem to have merged 0 and O together. Industry's meaning
of big O is closer to what academics mean by 0, in that it would be seen as incorrect to describe printing an
array as O(N2 ). Industry would just say this is O(N).
For this book, we will use big O in the way that industry tends to use it: By always trying to offer the tightest
description of the runtime.

Best, Worst & Expected
======================

Best Case: If all elements are equal, then quick sort will, on average, just traverse through the array once.
This is O ( N). (This actually depends slightly on the implementation of quick sort. There are implementations,
though, that will run very quickly on a sorted array.)

Worst Case: What if we get really unlucky and the pivot is repeatedly the biggest element in the array?
(Actually, this can easily happen. If the pivot is chosen to be the first element in the subarray and the
array is sorted in reverse order, we'll have this situation.) In this case, our recursion doesn't divide the
array in half and recurse on each half. It just shrinks the subarray by one element. This will degenerate
to an O(N2) runtime.

Expected Case: Usually, though, these wonderful or terrible situations won't happen. Sure, sometimes
the pivot will be very low or very high, but it won't happen over and over again. We can expect a runtime
of O(N log N).

We rarely ever discuss best case time complexity, because it's not a very useful concept. After all, we could
take essentially any algorithm, special case some input, and then get an O ( 1) time in the best case.
For many-probably most-algorithms, the worst case and the expected case are the same. Sometimes
they're different, though, and we need to describe both of the runtimes.

What is the relationship between best/worst/expected case and big O/theta/omega?
It's easy for candidates to muddle these concepts (probably because both have some concepts of"higher':
"lower" and "exactly right"), but there is no particular relationship between the concepts.
Best, worst, and expected cases describe the big O (or big theta) time for particular inputs or scenarios.

Big O, big omega, and big theta describe the upper, lower, and tight bounds for the runtime.

Example 13
===========
The following code computes the Nth Fibonacci number.
1 int fib(int n) {
2   if (n <= 0) return 0;
3   else if (n == 1) return 1;
4   return fib(n - 1) + fib(n - 2);
5 }
We can use the earlier pattern we'd established for recursive calls: O(branchesdepth).
There are 2 branches per call, and we go as deep as N, therefore the runtime is O(2 raisedTo N).

Through some very complicated math, we can actually get a tighter runtime. The time is indeed
exponential, but it's actually closer to O(1.6 raisedTo N). The reason that it's not exactly O(2 raisedTo N) is that, at
the bottom of the call stack, there is sometimes only one call. It turns out that a lot of the nodes
are at the bottom (as is true in most trees), so this single versus double call actually makes a big
difference. Saying O(2 raisedTo N) would suffice for the scope of an interview, though (and is still technically
correct, if you read the note about big theta on page 39). You might get "bonus points" if
you can recognize that it'll actually be less than that.

Generally speaking, when you see an algorithm with multiple recursive calls, you're looking at exponential
runtime.

Example14
==========
The following code prints all Fibonacci numbers from 0 to n. What is its time complexity?
1 void allFib(int n) {
2   for (int i= 0; i < n; i++) {
3     System.out.println(i + ": "+ fib(i));
4   }
5 }
6
7 int fib(int n) {
8   if (n <= 0) 
      return 0;
9   else if (n == 1) return 1;
10    return fib(n - 1) + fib(n - 2);
11 }
Many people will rush to concluding that since fib(n) takes O(2 rasiedTo n) time and it's called n times, then it's
O(n *(2 raisedTo n)).
Not so fast. Can you find the error in the logic?
The error is that the n is changing. Yes, fib (n) takes 0(2 raisedTo n) time, but it matters what that value of n is.
Instead, let's walk through each call.
fib(1) -> 2 raisedTo 1 steps
fib(2) -> 2 raisedTo 2 steps
fib(3) -> 2 raisedTo 3 steps
fib(4) -> 2 raisedTo 4 steps
fib(n) -> 2 raisedTo n steps
Therefore, the total amount of work is:
2 raisedTo 1 + 2 raisedTo 2 + 2 raisedTo 3 + 2 raisedTo 4 + , , , + 2 raisedTo n
As we showed on page 44, this is 2 raisedTo n+1. Therefore, the runtime to compute the first n Fibonacci numbers
(using this terrible algorithm) is still O( 2 raisedTo n).

Example 15
==========
The following code prints all Fibonacci numbers from 0 to n. However, this time, it stores (i.e., caches) previously
computed values in an integer array. If it has already been computed, it just returns the cache. What is its runtime?
1 void allFib(int n) {
2   int[] memo = new int[n + 1];
3   for (int i= 0; i < n; i++) {
4     System.out.println(i + ": "+ fib(i, memo));
5   }
6 }
7
8 int fib(int n, int[] memo) {
9   if (n <= 0) 
       return 0;
10  else if (n == 1)
       return 1;
11  else if (memo[n] > 0) 
       return memo[n];
13  memo[n] = fib(n - 1, memo)+ fib(n - 2, memo);     //memoization
14  return memo[n];
15 }
Let's walk through what this algorithm does.
fib(l) -> return 1
 fib(2)
  fib(l) -> return 1
  fib(0) -> return 0
  store 1 at memo[2]
fib(3)
  fib(2) -> lookup memo[2] -> return 1
  fib(l) -> return 1
  store 2 at memo[3]
fib(4)
  fib(3) -> lookup memo[3] -> return 2
  fib(2) -> lookup memo[2] -> return 1
  store 3 at memo[4]
fib(S)
  fib(4) -> lookup memo[4] -> return 3
  fib(3) -> lookup memo[3] -> return 2
  store 5 at memo[5]
  
At each call to fib(i), we have already computed and stored the values for fib(i-1) and fib(i-2).
We just look up those values, sum them, store the new result, and return. This takes a constant amount of
time.
We're doing a constant amount of work N times, so this is O (n) time.
This technique, called memoization, is a very common one to optimize exponential time recursive algorithms.
